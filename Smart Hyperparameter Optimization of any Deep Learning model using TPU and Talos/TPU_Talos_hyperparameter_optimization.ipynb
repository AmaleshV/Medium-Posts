{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU_Talos hyperparameter optimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmaleshV/Medium-Posts/blob/master/TPU_Talos_hyperparameter_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Cd6Xr_jeE2",
        "colab_type": "text"
      },
      "source": [
        "#  Increase RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfx-uJ9IijLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=[]\n",
        "while(1):\n",
        "  d.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzihCLzjuoq",
        "colab_type": "text"
      },
      "source": [
        "**Remove any pre-defined variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoAjJXuDj5ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove all defined variables\n",
        "%reset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtPlhOQzj72e",
        "colab_type": "text"
      },
      "source": [
        "# TPU + Talos Pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXOIcd08kDJX",
        "colab_type": "text"
      },
      "source": [
        "**Upload Kaggle JSON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5TIY7edj_2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "df = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mxCmm9Fy7yx",
        "colab_type": "text"
      },
      "source": [
        "**Update kaggle api**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg4r3DR8y_pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGXZeFEklpBU",
        "colab_type": "text"
      },
      "source": [
        "**Libraries Used**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSKSq64kIFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install required packages\n",
        "!pip install git+https://github.com/autonomio/talos\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Dense\n",
        "import keras.backend as K\n",
        "import talos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OIw8mwIl0dM",
        "colab_type": "text"
      },
      "source": [
        "# 1. Downloading the datasets from API calls**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3BhsdFQl3O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download --force ieee-fraud-detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYG2KlmzyDzk",
        "colab_type": "text"
      },
      "source": [
        "# 2.Pre-Processing and Data Wrangling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMCGISYJyGFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "import dask.dataframe as dd\n",
        "train_identity = dd.read_csv('train_identity.csv')\n",
        "train_transaction = dd.read_csv('train_transaction.csv')\n",
        "test_identity = dd.read_csv('test_identity.csv')\n",
        "test_transaction = dd.read_csv('test_transaction.csv')\n",
        "sub = dd.read_csv('sample_submission.csv')\n",
        "# let's combine the data and work with the whole dataset\n",
        "train = dd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
        "test = dd.merge(test_transaction, test_identity, on='TransactionID', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT7Km4rwzFAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to pandas\n",
        "train = train.compute()\n",
        "test = test.compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCf7p23TzHvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Wrangling\n",
        "df=train\n",
        "df=df.sample(frac=1,random_state=123) #shuffle data for memory bias\n",
        "#from sklearn import preprocessing\n",
        "# Identify which variables can be numeric and which factor (<32 uniq levels)\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "cat_cols= [col for col in df.columns if (df[col].dtypes==object) & (df[col].nunique() > 32)]\n",
        "print(cat_cols)\n",
        "df= df.drop(cat_cols, axis=1)\n",
        "#df= df.loc[:,(df.dtypes== 'int64') | (df.dtypes==float) |((df.dtypes==object) & (df.nunique() < 32))]\n",
        "# Select only columns with less than 32 unique values if object or select numeric variables\n",
        "df_wrangled=df\n",
        "df_wrangled=train\n",
        "if 'isFraud' in df_wrangled:\n",
        "        df_wrangled = df_wrangled.loc[df_wrangled['isFraud'].notnull(),:] #pick only those alerts with target variable defined\n",
        "print(df_wrangled.head())\n",
        "# # Train Test Split\n",
        "# set x and y variables\n",
        "if 'isFraud' in df_wrangled:\n",
        "    y = df.isFraud\n",
        "    x= df.loc[:, df.columns != 'isFraud']\n",
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lROtv5PuzI72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Training Data preprocessing\n",
        "# Eliminate automatically variables with more than 20% of missingness\n",
        "xTrain = xTrain[xTrain.columns[xTrain.isnull().mean() < 0.2]]\n",
        "# fill missing values with mean column values for numeric features\n",
        "#xTrain.select_dtypes(include=numerics) =  xTrain.select_dtypes(include=numerics).fillna(xTrain.mean(), inplace=True)\n",
        "#xTrain._get_numeric_data().fillna(xTrain.mean(), inplace=True)\n",
        "cols= xTrain.columns\n",
        "colnames_numerics_only = xTrain.select_dtypes(include=np.number).columns.tolist()\n",
        "colnames_numerics_only\n",
        "xTrain[colnames_numerics_only] = xTrain[colnames_numerics_only].fillna(xTrain[colnames_numerics_only].mean())\n",
        "# impute categorical variables with mode\n",
        "#cols = xTrain.select_dtypes(include= ).columns\n",
        "cat_cols= list(set(cols) - set(colnames_numerics_only))\n",
        "#xTrain[cat_cols] = xTrain.loc[:,xTrain.columns.isin(cat_cols) & (xTrain.isna().any())].fillna(xTrain[cat_cols].mode().iloc[0])\n",
        "xTrain[cat_cols] = xTrain[cat_cols].fillna(xTrain[cat_cols].mode().iloc[0])\n",
        "train_cols = xTrain.columns\n",
        "#xTrain.loc[:,(xTrain.dtypes==object)] = xTrain.loc[:,(xTrain.dtypes==object)].fillna(xTrain.mode())\n",
        "# get categorical features index\n",
        "xTrain.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR25J8hezO-Z",
        "colab_type": "text"
      },
      "source": [
        "# 3. Feature Engineering and Feature Selection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVJW0jeszWgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "##change to GPU before running this code chunk and save the final_selected_cols as binary columns object so that you can use feature for later deep learning model\n",
        "xTrain_dummy = pd.get_dummies(xTrain, prefix_sep='_', drop_first=True) #create dummies\n",
        "xgb_cols =xTrain_dummy.columns\n",
        "xTrain_xgb=xTrain_dummy\n",
        "yTrain_xgb = yTrain\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from xgboost import XGBClassifier\n",
        "xgbc=XGBClassifier(n_estimators=500,verbose=1,tree_method='gpu_hist',random_state=123)\n",
        "embeded_xgb_selector = SelectFromModel(xgbc)\n",
        "embeded_xgb_selector.fit(xTrain_xgb, yTrain_xgb)\n",
        "embeded_xgb_support = embeded_xgb_selector.get_support()\n",
        "embeded_xgb_feature = xTrain_xgb.loc[:,embeded_xgb_support].columns.tolist()\n",
        "final_selected_cols=embeded_xgb_feature\n",
        "final_tr=xTrain_xgb[embeded_xgb_feature]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdK5iNV_z3W7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Test Data preprocessing ###\n",
        "# Eliminate automatically variables with more than 20% of missingness\n",
        "xTest = xTest[xTest.columns[xTest.isnull().mean() < 0.2]]\n",
        "# fill missing values with mean column values for numeric features\n",
        "test_cols= xTest.columns\n",
        "num_cols = xTest.select_dtypes(include=np.number).columns\n",
        "xTest[num_cols] = xTest[num_cols].fillna(xTest[num_cols].mean())\n",
        "\n",
        "# impute categorical variables with mode\n",
        "#cols = xTest.select_dtypes(include= ).columns\n",
        "cat_cols= list(set(test_cols) - set(num_cols))\n",
        "#xTest[cat_cols] = xTest.loc[:,xTest.columns.isin(cat_cols) & (xTest.isna().any())].fillna(xTest[cat_cols].mode().iloc[0])\n",
        "xTest.loc[:,cat_cols] = xTest[cat_cols].fillna(xTest[cat_cols].mode().iloc[0])\n",
        "\n",
        "# Select only those features which are there in training #\n",
        "#train_cols = xTrain.columns\n",
        "xTest = xTest[train_cols] \n",
        "\n",
        "\n",
        "\n",
        "# Dummify categorical vars\n",
        "xTest_dummy = pd.get_dummies(xTest, prefix_sep='_', drop_first=True)\n",
        "\n",
        "##missing columns levels train and test\n",
        "missing_levels_cols= list(set(xTrain_dummy.columns) - set(xTest_dummy.columns))\n",
        "\n",
        "\n",
        "for c in missing_levels_cols:\n",
        "    xTest_dummy[c]=0\n",
        "\n",
        "\n",
        "# Select only those variables in final_tr\n",
        "final_ts =xTest_dummy[final_selected_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYfiVZzRzsg9",
        "colab_type": "text"
      },
      "source": [
        "# 4. Scaling the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tklehvgPzvNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler  \n",
        "scaler = StandardScaler()  \n",
        "# fit and transform only on training data  \n",
        "final_tr_scaled = scaler.fit_transform(final_tr)  \n",
        "### Apply on test data ###\n",
        "final_ts_scaled = scaler.transform(final_ts) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0LPpl4K0Abc",
        "colab_type": "text"
      },
      "source": [
        "# 5. TPU Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F8_-yAd0CQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_host(resolver.master())\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suA0c7TF0KOn",
        "colab_type": "text"
      },
      "source": [
        "# 6. Model Building:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfCYh0ld0MVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ieee_fraud_model(x_tr, y_tr, x_ts, y_ts, params):\n",
        "# Specify a distributed strategy to use TPU\n",
        "  with strategy.scope():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(params['first_hidden_layer'], input_dim=final_tr.shape[1],\n",
        "                            activation=params['activation1'],\n",
        "                            kernel_initializer=params['kernel_initializer']))\n",
        "    model.add(Dropout(params['dropout']))\n",
        "    model.add(Dense(params['second_hidden_layer'], \n",
        "                            activation=params['activation2'],\n",
        "                            use_bias=True))\n",
        "    \n",
        "    model.add(Dense(1, activation=params['last_activation']))\n",
        "    \n",
        "    model.compile(loss=params['losses'],\n",
        "                            optimizer=params['optimizer'],\n",
        "                            metrics=['binary_accuracy'])\n",
        "  \n",
        "  x_tr = np.asarray(x_tr).astype('float32')\n",
        "  y_tr = np.asarray(y_tr).astype('float32').reshape((-1,1))\n",
        "  x_ts = np.asarray(x_ts).astype('float32')\n",
        "  y_ts = np.asarray(y_ts).astype('float32').reshape((-1,1))\n",
        "  # Fit the Keras model on the dataset\n",
        "  steps_per_epoch = int(np.ceil(x_tr.shape[0] / params['batch_size'])) - 1\n",
        "  history = model.fit(x_tr, y_tr, \n",
        "                            validation_data=[x_ts, y_ts],\n",
        "                            batch_size=params['batch_size'],\n",
        "                            epochs=params['epochs'],\n",
        "                          callbacks=[talos.utils.live()],\n",
        "                            verbose=1)\n",
        "  return history, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf-Hkymt0TR2",
        "colab_type": "text"
      },
      "source": [
        "# 7. Parameter Grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5xfZzKx0VBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = {'activation1':[ tf.nn.elu,tf.nn.relu],\n",
        "     'activation2':[ tf.nn.elu,tf.nn.relu],\n",
        "     'first_hidden_layer': [10,8],\n",
        "     'second_hidden_layer': [6],\n",
        "     'batch_size': [100, 400],\n",
        "     'epochs': [30],\n",
        "     'learn_rate': [0.1,0.01],\n",
        "     'momentum': [0.2],\n",
        "     'dropout': [0, 0.1],\n",
        "     'weight_regulizer':[None],\n",
        "     'emb_output_dims': [None],\n",
        "     'optimizer': [ 'sgd','Adam', 'Nadam', 'RMSprop'],   \n",
        "     'losses': ['binary_crossentropy','logcosh'],\n",
        "     'last_activation': ['sigmoid'],\n",
        "     'kernel_initializer':[\"GlorotUniform\"]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj0CdCIO0tdu",
        "colab_type": "text"
      },
      "source": [
        "# 8. Talos hyperparameter scanning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWCUD3xy0v8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "%matplotlib inline\n",
        "from keras.models import load_model\n",
        "from keras.utils import CustomObjectScope\n",
        "from keras.initializers import glorot_uniform\n",
        "import os\n",
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "  ty = talos.Scan(x=final_tr_scaled,\n",
        "               y=yTrain,x_val=final_ts_scaled,y_val=yTest,\n",
        "               model=ieee_fraud_model,\n",
        "               params=p,\n",
        "               experiment_name='Talos_metric_ba')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdyTun5Y1VlH",
        "colab_type": "text"
      },
      "source": [
        "# 9. Prediction and submission file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8piend011zxd",
        "colab_type": "text"
      },
      "source": [
        "**Dataframe of all the models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3h5NKWJ1X-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = talos.Analyze(ty)\n",
        "# access the dataframe with the results\n",
        "output_dataframe=analyze_object.data\n",
        "# get the highest result for any metric\n",
        "print(analyze_object.high('binary_accuracy'))\n",
        "output_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzZ9S2hC16xb",
        "colab_type": "text"
      },
      "source": [
        "**Prediction according to metric**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFjneIxO1-Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "  talospred=talos.Predict(scan_object=ty)\n",
        "  y_predi=talospred.predict(x=final_ts_scaled,metric='binary_accuracy',asc=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK0iVVB62Cqw",
        "colab_type": "text"
      },
      "source": [
        "**Deploying the Best model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etE8vscS2H5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "  talos.Deploy(scan_object=ty, model_name='tal_model',metric='val_binary_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9_jaVfC2MLW",
        "colab_type": "text"
      },
      "source": [
        "**Restoring the best saved model from talos for predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nln6DPqG2S9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#restore the saved model\n",
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "  tal_model = talos.Restore('tal_model.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz9p-0pu2e1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####submission prediction\n",
        "### Test Data preprocessing ###\n",
        "# Eliminate automatically variables with more than 20% of missingness\n",
        "test = test[test.columns[test.isnull().mean() < 0.2]]\n",
        "# fill missing values with mean column values for numeric features\n",
        "test_cols= test.columns\n",
        "num_cols = test.select_dtypes(include=np.number).columns\n",
        "test[num_cols] = test[num_cols].fillna(test[num_cols].mean())\n",
        "\n",
        "# impute categorical variables with mode\n",
        "#cols = .select_dtypes(include= ).columns\n",
        "cat_cols= list(set(test_cols) - set(num_cols))\n",
        "#test[cat_cols] = test.loc[:,test.columns.isin(cat_cols) & (test.isna().any())].fillna(test[cat_cols].mode().iloc[0])\n",
        "test.loc[:,cat_cols] = test[cat_cols].fillna(test[cat_cols].mode().iloc[0])\n",
        "\n",
        "# Select only those features which are there in training #\n",
        "#train_cols = xTrain.columns\n",
        "test = test[train_cols] \n",
        "\n",
        "\n",
        "\n",
        "# Dummify categorical vars\n",
        "test_dummy = pd.get_dummies(test, prefix_sep='_', drop_first=True)\n",
        "\n",
        "##missing columns levels train and test\n",
        "missing_levels_cols= list(set(xTrain_dummy.columns) - set(test_dummy.columns))\n",
        "\n",
        "\n",
        "for c in missing_levels_cols:\n",
        "    test_dummy[c]=0\n",
        "\n",
        "\n",
        "# Select only those variables in final_tr\n",
        "final_ts1 =test_dummy[final_selected_cols]\n",
        "final_ts1_scaled = scaler.transform(final_ts1)\n",
        "\n",
        "#predictions from the best talos model\n",
        "ytest_seq=tal_model.model.predict(final_ts1_scaled)\n",
        "### prepping submission file###\n",
        "submission_seq= np.c_[test.TransactionID,ytest_seq]\n",
        "submission_seq= pd.DataFrame(submission_seq)\n",
        "submission_seq.columns= ['TransactionID','isFraud']\n",
        "print(submission_seq)\n",
        "submission_seq.to_csv('submission_seq.csv')\n",
        "from google.colab import files\n",
        "files.download('submission_seq.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BDQkGp6W6Ng",
        "colab_type": "text"
      },
      "source": [
        "# For only Talos with CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTbpzUUWXDuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ieee_fraud_model(x_tr, y_tr, x_ts, y_ts, params):\n",
        "# Specify a distributed strategy to use TPU\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(params['first_hidden_layer'], input_dim=final_tr.shape[1],\n",
        "                            activation=params['activation1'],\n",
        "                            kernel_initializer=params['kernel_initializer']))\n",
        "  model.add(tf.keras.layers.Dense(params['second_hidden_layer'], \n",
        "                            activation=params['activation2'],\n",
        "                            kernel_initializer=params['kernel_initializer'] ,\n",
        "                            use_bias=True))\n",
        "  model.add(tf.keras.layers.Dense(params['third_hidden_layer'], \n",
        "                            activation=params['activation3'], \n",
        "                            use_bias=True,\n",
        "                            kernel_initializer=params['kernel_initializer']))\n",
        "  model.add(tf.keras.layers.Dense(1, activation=params['last_activation']))\n",
        "  optimizer = tf.keras.optimizers.SGD(lr=params['learn_rate'], momentum=params['momentum'])\n",
        "  model.compile(loss=params['losses'],\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=['binary_accuracy'])\n",
        "                      \n",
        "  \n",
        "  x_tr = np.asarray(x_tr).astype('float32')\n",
        "  y_tr = np.asarray(y_tr).astype('float32').reshape((-1,1))\n",
        "  x_ts = np.asarray(x_ts).astype('float32')\n",
        "  y_ts = np.asarray(y_ts).astype('float32').reshape((-1,1))\n",
        "  # Fit the Keras model on the dataset\n",
        "  steps_per_epoch = int(np.ceil(x_tr.shape[0] / params['batch_size'])) - 1\n",
        "  history = model.fit(x_tr, y_tr, \n",
        "                            validation_data=[x_ts, y_ts],\n",
        "                            batch_size=params['batch_size'],\n",
        "                            epochs=params['epochs'],steps_per_epoch=steps_per_epoch,\n",
        "                          callbacks=[talos.utils.live()],\n",
        "                            verbose=1)\n",
        "  return history, model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}